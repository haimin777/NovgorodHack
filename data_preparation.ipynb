{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0996f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textract\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e968b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/media/haimin/Toshiba_lib/Corruption/dataset/DataSet_Razmetra/Алтайский край/3_1/311AEF5C-C2EF-42D1-92D8-6FE47868D4DE/Edition_1/Edition_Text.docx',\n",
       "  '/media/haimin/Toshiba_lib/Corruption/dataset/DataSet_Razmetra/Алтайский край/3_1/8635893F-2960-4FB2-BB81-D765C0A51510/Edition_1/Edition_Text.docx'],\n",
       " 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = glob.glob('/media/haimin/Toshiba_lib/Corruption/dataset/DataSet_Razmetra/Алтайский край/*/*/*/Ed*')\n",
    "docs[:2], len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c33f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashname(path, target_dirname='DataSet_Razmetra'):\n",
    "    \n",
    "    hashname = path.split(target_dirname)[1].split('/')[3]\n",
    "    \n",
    "    return hashname\n",
    "\n",
    "\n",
    "def get_tagret_sentences(doc_path):\n",
    "    byte_text = textract.process(doc_path)\n",
    "    text = byte_text.decode() \n",
    "    # split to sentenses\n",
    "    p = re.compile('[0-9]. ') # to find all numbered points\n",
    "\n",
    "    #s = re.split(r'[.?!:]+', text)\n",
    "    s = text.split('\\n') # separate on paragraph\n",
    "    target_sentencse = []\n",
    "    for sentense in s:\n",
    "        if p.match(sentense):\n",
    "            #print(sentense)\n",
    "            target_sentencse.append(sentense)\n",
    "            \n",
    "    return target_sentencse         \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def create_hashed_paragraphs(docs_path):\n",
    "    result_dict = {}\n",
    "    for path in tqdm(docs_path):\n",
    "        hashname = get_hashname(path)\n",
    "        target_sentences = get_tagret_sentences(path)\n",
    "        result_dict[hashname] = target_sentences\n",
    "        \n",
    "    return  result_dict   \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b60d176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:01<00:00, 17.23it/s]\n"
     ]
    }
   ],
   "source": [
    "source_docs = create_hashed_paragraphs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99012fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/media/haimin/Toshiba_lib/Corruption/dataset/DataSet_Razmetra/Архангельская область/3_9/CE2BE86C-E48B-4DAC-881C-FD0EFC6FA8A4/Edition_1/Edition_1/Expertise_Text.docx',\n",
       "  '/media/haimin/Toshiba_lib/Corruption/dataset/DataSet_Razmetra/Ненецкий автономный округ/4_2/1E35A010-A810-45B8-A286-4F03B4D2E3E9/288ED52A-0803-42E2-8530-13EB71560639/Edition_1/Expertise_Text.docx'],\n",
       " 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate target points from expertize texts\n",
    "expertises = glob.glob('/media/haimin/Toshiba_lib/Corruption/dataset/DataSet_Razmetra/*/*/*/*/*/Ex*') \n",
    "expertises[:2], len(expertises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c3cea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(word, sentences):\n",
    "       #return [i for i in sentences if re.search(r'\\b%s\\b' % word, i)]\n",
    "    return [i for i in sentences if word in i]\n",
    "\n",
    "def get_tagret_sentences_fromExpertize(expertize_path):\n",
    "    \n",
    "    points = [] # пункты, которые упоминаются в экспертизе, как проблемные\n",
    "    byte_text = textract.process(expertize_path)\n",
    "    text = byte_text.decode() \n",
    "    # split to sentenses\n",
    "    sentenses = re.split(r'[.?!:]+', text)\n",
    "    #find target sentenses\n",
    "    target_sentenses = search('пункт', sentenses)\n",
    "    counter = False\n",
    "    for i in target_sentenses:\n",
    "        for p in i.split(' '):\n",
    "    \n",
    "            if 'пунк' in p:\n",
    "                points.append(p)\n",
    "                counter = True\n",
    "            elif counter:\n",
    "                points.append(p)\n",
    "                counter = False\n",
    "        \n",
    "    return points\n",
    "\n",
    "\n",
    "def create_hashed_Expertize(paths): # separate numbers of problem points from expertize files\n",
    "    result_dict = {}\n",
    "    for path in tqdm(paths):\n",
    "        hashname = get_hashname(path) # from folder name\n",
    "        problem_points = get_tagret_sentences_fromExpertize(path)\n",
    "        result_dict[hashname] = problem_points\n",
    "        \n",
    "    return   result_dict  \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17f82642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CE2BE86C-E48B-4DAC-881C-FD0EFC6FA8A4': ['пунктом',\n",
       "  '2',\n",
       "  'пункту',\n",
       "  '16',\n",
       "  'пункта',\n",
       "  '17',\n",
       "  'пункте',\n",
       "  '15',\n",
       "  'подпунктах',\n",
       "  '1',\n",
       "  'пункта',\n",
       "  '16',\n",
       "  'пунктом',\n",
       "  '20',\n",
       "  'подпункту',\n",
       "  '2',\n",
       "  'пункта',\n",
       "  '20',\n",
       "  'пунктами',\n",
       "  '14',\n",
       "  'подпунктами',\n",
       "  '1,',\n",
       "  'пункта',\n",
       "  '16',\n",
       "  'пункте',\n",
       "  '16',\n",
       "  'подпунктом',\n",
       "  '«и»',\n",
       "  'пункта',\n",
       "  '3',\n",
       "  'подпункту',\n",
       "  '«б»',\n",
       "  'пункта',\n",
       "  '4',\n",
       "  'подпункт',\n",
       "  '2',\n",
       "  'пункта',\n",
       "  '20',\n",
       "  'подпункте',\n",
       "  '2',\n",
       "  'пункта',\n",
       "  '20',\n",
       "  'пункт',\n",
       "  '16',\n",
       "  'подпункт',\n",
       "  '2',\n",
       "  'пункта',\n",
       "  '16'],\n",
       " '1E35A010-A810-45B8-A286-4F03B4D2E3E9': ['пункту',\n",
       "  '47',\n",
       "  'пункта',\n",
       "  '108',\n",
       "  'пункте',\n",
       "  '108',\n",
       "  'пункту',\n",
       "  '17',\n",
       "  'пунктом',\n",
       "  '2',\n",
       "  'подпункту',\n",
       "  '1',\n",
       "  'пункта',\n",
       "  '24',\n",
       "  'пункте',\n",
       "  '20',\n",
       "  'пунктом',\n",
       "  '20',\n",
       "  'пункту',\n",
       "  '20',\n",
       "  'пункте',\n",
       "  '20',\n",
       "  'пункте',\n",
       "  '19',\n",
       "  'пунктом',\n",
       "  '20',\n",
       "  'пункте',\n",
       "  '20',\n",
       "  'подпунктом',\n",
       "  '«и»',\n",
       "  'пункта',\n",
       "  '3',\n",
       "  'подпункту',\n",
       "  '«б»',\n",
       "  'пункта',\n",
       "  '4',\n",
       "  'подпункт',\n",
       "  '1',\n",
       "  'пункта',\n",
       "  '24',\n",
       "  'подпункте',\n",
       "  '1',\n",
       "  'пункта',\n",
       "  '24']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_hashed_Expertize(expertises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b04df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
